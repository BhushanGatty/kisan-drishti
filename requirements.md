# Requirements Document: Kisan-Drishti (Farmer's Vision)

## Introduction

Kisan-Drishti is a vernacular-first, agentic robotics bridge system designed for the "AI for Bharat" Hackathon. The system enables rural farmers to control an autonomous strawberry-picking robot through local language voice notes via WhatsApp. The robot operates in a simulated MuJoCo environment using the Hello Robot Stretch 2 model, with Google Antigravity providing autonomous task planning and execution capabilities.

## Glossary

- **System**: The complete Kisan-Drishti platform including all components
- **Voice_Interface**: The WhatsApp-based voice note input/output system
- **Speech_To_Intent_Engine**: AI component that converts vernacular speech to technical intents
- **Antigravity_Agent**: Google Antigravity-powered autonomous agent for mission planning
- **MuJoCo_Simulator**: Physics engine for robot simulation and validation
- **Robot_Controller**: Component managing the 8-DOF Hello Robot Stretch 2 actuators
- **Mission_Planner**: Subcomponent of Antigravity Agent that generates trajectory plans
- **Feedback_Generator**: Component that captures and sends visual confirmation to farmers
- **n8n_Orchestrator**: API bridge hosted on Ubuntu coordinating all system components
- **WhatsApp_Gateway**: Wassenger/Twilio integration for WhatsApp communication
- **Farmer**: End user who sends voice commands and receives feedback
- **Administrator**: System operator who manages and monitors the platform

## Requirements

### Requirement 1: Vernacular Voice Input Processing

**User Story:** As a farmer, I want to send voice notes in my local language (Hindi, Marathi, etc.) via WhatsApp, so that I can control the robot without learning technical terminology or English.

#### Acceptance Criteria

1. WHEN a voice note is received via WhatsApp, THE Voice_Interface SHALL forward it to the Speech_To_Intent_Engine within 2 seconds
2. WHEN the Speech_To_Intent_Engine processes a voice note, THE System SHALL support Hindi, Marathi, and at least 3 other Indian regional languages
3. WHEN a voice note contains a picking command in any supported language, THE Speech_To_Intent_Engine SHALL extract the target location and action intent
4. IF a voice note is unclear or contains unsupported language, THEN THE System SHALL send a clarification request back to the farmer in their detected language
5. WHEN intent extraction is complete, THE System SHALL generate a structured command object containing action type, target coordinates, and confidence score

### Requirement 2: Agentic Mission Planning

**User Story:** As a farmer, I want the robot to autonomously plan how to pick strawberries based on my simple voice command, so that I don't need to specify detailed movement instructions.

#### Acceptance Criteria

1. WHEN a structured command object is received, THE Antigravity_Agent SHALL generate a complete trajectory plan within 5 seconds
2. WHEN generating a trajectory plan, THE Mission_Planner SHALL calculate optimal actuator sequences for all 8 DOF (base forward/turn, lift, arm extend, wrist yaw, grip, head pan/tilt)
3. WHEN planning a pick operation, THE Mission_Planner SHALL include approach, grasp, and retract phases
4. IF the target location is unreachable, THEN THE Antigravity_Agent SHALL notify the farmer with an explanation and suggest alternative actions
5. WHEN a trajectory plan is complete, THE Antigravity_Agent SHALL generate executable Python code for MuJoCo simulation

### Requirement 3: Robot Simulation and Control

**User Story:** As a system administrator, I want the robot to execute picking operations in a physics-accurate simulation, so that I can validate the system before deploying to physical hardware.

#### Acceptance Criteria

1. WHEN Python code is generated by the Antigravity_Agent, THE Robot_Controller SHALL execute it in the MuJoCo_Simulator
2. WHEN controlling the robot, THE Robot_Controller SHALL map commands to the correct actuator indices (0: Forward, 1: Turn, 2: Lift, 3: Arm Extend, 4: Wrist Yaw, 5: Grip, 6: Head Pan, 7: Head Tilt)
3. WHEN the gripper reaches a strawberry, THE System SHALL apply a weld constraint (Magnet Pick logic) to attach the object
4. WHEN executing a trajectory, THE MuJoCo_Simulator SHALL validate physics constraints in real-time and report any collisions or constraint violations
5. WHEN a picking operation completes successfully, THE Robot_Controller SHALL return the final robot state and gripper status

### Requirement 4: Visual Feedback Generation

**User Story:** As a farmer, I want to receive a photo showing the robot has picked the strawberry, so that I can confirm the task was completed successfully.

#### Acceptance Criteria

1. WHEN a picking operation completes, THE Feedback_Generator SHALL capture an RGB screenshot from the robot's head camera
2. WHEN capturing the screenshot, THE System SHALL ensure the gripper and picked object are visible in the frame
3. WHEN the screenshot is captured, THE Feedback_Generator SHALL overlay mission status text in the farmer's language
4. WHEN the feedback image is ready, THE Voice_Interface SHALL send it to the farmer via WhatsApp within 3 seconds
5. WHEN sending feedback, THE System SHALL include a voice message confirming task completion in the farmer's language

### Requirement 5: WhatsApp Communication Gateway

**User Story:** As a farmer, I want to interact with the robot entirely through WhatsApp, so that I can use a familiar platform without installing new apps.

#### Acceptance Criteria

1. WHEN a farmer sends a message, THE WhatsApp_Gateway SHALL authenticate the user and validate their permissions
2. WHEN receiving media, THE WhatsApp_Gateway SHALL support voice notes in common audio formats (AAC, MP3, OGG)
3. WHEN sending responses, THE WhatsApp_Gateway SHALL deliver images and voice messages through Wassenger or Twilio APIs
4. IF the WhatsApp_Gateway connection fails, THEN THE System SHALL retry with exponential backoff up to 3 attempts
5. WHEN a session is active, THE WhatsApp_Gateway SHALL maintain conversation context for follow-up commands

### Requirement 6: n8n Orchestration Layer

**User Story:** As a system administrator, I want a centralized orchestration system that coordinates all components, so that I can monitor and debug the entire pipeline.

#### Acceptance Criteria

1. WHEN a voice note arrives, THE n8n_Orchestrator SHALL route it through the complete processing pipeline (Voice_Interface → Speech_To_Intent_Engine → Antigravity_Agent → Robot_Controller → Feedback_Generator)
2. WHEN coordinating components, THE n8n_Orchestrator SHALL log all API calls, responses, and timestamps
3. WHEN a component fails, THE n8n_Orchestrator SHALL capture the error, notify the administrator, and send a user-friendly message to the farmer
4. WHEN processing a request, THE n8n_Orchestrator SHALL enforce a maximum end-to-end latency of 15 seconds
5. WHERE monitoring is enabled, THE n8n_Orchestrator SHALL expose metrics for request count, success rate, and average latency

### Requirement 7: Multi-Language Support

**User Story:** As a farmer, I want the system to understand and respond in my preferred Indian language, so that I can communicate naturally without language barriers.

#### Acceptance Criteria

1. WHEN a farmer first interacts with the system, THE System SHALL detect their language from the voice note
2. WHEN language is detected, THE System SHALL persist the preference for future interactions
3. WHEN generating responses, THE System SHALL use the farmer's preferred language for all text and voice messages
4. WHEN translating technical terms, THE System SHALL use culturally appropriate terminology (e.g., "strawberry" → "स्ट्रॉबेरी" in Hindi)
5. WHERE language detection confidence is below 70%, THE System SHALL ask the farmer to confirm their language preference

### Requirement 8: Error Handling and Recovery

**User Story:** As a system administrator, I want the system to handle failures gracefully and provide clear error messages, so that farmers can understand what went wrong and how to proceed.

#### Acceptance Criteria

1. IF the Speech_To_Intent_Engine fails to process a voice note, THEN THE System SHALL ask the farmer to repeat their command more clearly
2. IF the MuJoCo_Simulator detects a collision during execution, THEN THE System SHALL abort the operation and notify the farmer with a visual explanation
3. IF the Antigravity_Agent cannot generate a valid plan, THEN THE System SHALL explain the constraint violation in simple terms
4. WHEN any component times out, THE System SHALL send a status update to the farmer indicating the system is still processing
5. WHEN a critical error occurs, THE System SHALL log detailed diagnostics and alert the administrator via configured channels

### Requirement 9: Robot Actuator Control

**User Story:** As a system administrator, I want precise control over all 8 degrees of freedom of the Hello Robot Stretch 2, so that the robot can perform complex picking maneuvers.

#### Acceptance Criteria

1. WHEN controlling the base, THE Robot_Controller SHALL use actuator 0 for forward/backward movement and actuator 1 for rotation
2. WHEN adjusting height, THE Robot_Controller SHALL use actuator 2 (Lift) to move the vertical mast
3. WHEN reaching for objects, THE Robot_Controller SHALL use actuator 3 (Arm Extend) for prismatic extension
4. WHEN orienting the gripper, THE Robot_Controller SHALL use actuator 4 (Wrist Yaw) for rotation
5. WHEN grasping objects, THE Robot_Controller SHALL use actuator 5 (Grip) to open and close the gripper
6. WHEN positioning the camera, THE Robot_Controller SHALL use actuators 6 (Head Pan) and 7 (Head Tilt)
7. WHEN executing movements, THE Robot_Controller SHALL respect joint limits and velocity constraints defined in the MuJoCo model

### Requirement 10: Session Management and Context

**User Story:** As a farmer, I want to have multi-turn conversations with the robot, so that I can refine commands or issue follow-up instructions without repeating context.

#### Acceptance Criteria

1. WHEN a farmer starts a conversation, THE System SHALL create a session with a unique identifier
2. WHEN processing commands, THE System SHALL maintain context including previous commands, robot state, and picked objects
3. WHEN a farmer says "pick another one", THE System SHALL infer the action type from previous context
4. WHEN a session is inactive for 10 minutes, THE System SHALL close it and clear the context
5. WHERE a farmer references a previous action, THE System SHALL resolve the reference using conversation history
